{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset from https://ai.stanford.edu/~amaas/data/sentiment/"
      ],
      "metadata": {
        "id": "oRMi3iDy88Hs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0WXLg4RxT_q"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import os\n",
        "filename = \"aclImdb_v1.tar.gz\"\n",
        "extraction_path = \"/content/dataset/\"\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "with tarfile.open(filename, 'r:gz') as tar:\n",
        "    tar.extractall(path=extraction_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Function to load the dataset from a given folder\n",
        "def load_dataset(folder):\n",
        "    data = []\n",
        "    for label in [\"pos\", \"neg\"]:\n",
        "        label_path = os.path.join(extraction_path, \"aclImdb\", folder, label)\n",
        "        for file in os.listdir(label_path):\n",
        "            with open(os.path.join(label_path, file), \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "                data.append({\"text\": text, \"label\": label})\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Load the training and testing datasets\n",
        "train_data = load_dataset(\"train\")\n",
        "test_data = load_dataset(\"test\")\n"
      ],
      "metadata": {
        "id": "w8h1UkC7xZSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the training dataset\n",
        "print(\"Training Dataset:\")\n",
        "print(train_data.head(1))\n",
        "\n",
        "# Display the first few rows of the testing dataset\n",
        "print(\"\\nTesting Dataset:\")\n",
        "print(test_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdYE4wcTxoA9",
        "outputId": "6ff15e8c-d86e-46e8-dd87-3112216b4fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset:\n",
            "                                                text label\n",
            "0  Street Fight is a brilliant piece of brutal sa...   pos\n",
            "\n",
            "Testing Dataset:\n",
            "                                                text label\n",
            "0  This film was original in an unoriginal way. A...   pos\n",
            "1  An extremely dark and brooding show with an ex...   pos\n",
            "2  First off, I absolutely loved this movie. As a...   pos\n",
            "3  This is not the kind of movie that really meri...   pos\n",
            "4  Julie Andrews and Rock Hudson were great in th...   pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of labels in the training dataset\n",
        "print(\"Training Label Distribution:\")\n",
        "print(train_data['label'].value_counts())\n",
        "\n",
        "# Check the distribution of labels in the testing dataset\n",
        "print(\"\\nTesting Label Distribution:\")\n",
        "print(test_data['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZTm2Jtvxpu5",
        "outputId": "afee8644-07c7-41aa-c6e9-e2112ce266f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Label Distribution:\n",
            "pos    12500\n",
            "neg    12500\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Testing Label Distribution:\n",
            "pos    12500\n",
            "neg    12500\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a positive review\n",
        "print(\"Positive Review Example:\")\n",
        "print(train_data[train_data['label'] == 'pos'].iloc[0]['text'])\n",
        "\n",
        "# Display a negative review\n",
        "print(\"\\nNegative Review Example:\")\n",
        "print(train_data[train_data['label'] == 'neg'].iloc[0]['text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXpz51IDxsZ4",
        "outputId": "d312fe54-c594-4b46-e74e-eb6f880b5253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Review Example:\n",
            "Street Fight is a brilliant piece of brutal satire. This is not a movie you just watch for fun. It is not a comfortable experience, although it does have some laugh-out-loud moments. This is a movie you watch when you need food for thought.<br /><br />To dismiss this film as simply racist is to miss the point entirely. This is not only a satire of Song of the South, it's also a biting commentary on the prejudices that Americans still have as a society. Every ethnic group portrayed in the movie gets shown as grotesque caricatures of their stereotypes, which in turn are grotesque caricatures of real people. Through this wild exaggeration, the filmmaker shows just how absurd these tightly-held beliefs really are.<br /><br />If you're the sort of person who's willing to acknowledge the ugliness of the prevalent prejudices American culture still holds, and if you're not afraid to look your own prejudices in the eye, this movie may be for you.\n",
            "\n",
            "Negative Review Example:\n",
            "\"Washington Square\" is a flat, shabby adaptation of the short novel by Henry James. Indeed, the novel is very good, but far from the level of James' masterpieces. Moreover its simple, eventless story seems unsuited to make it into a film (although William Wyler, with his \"The Heiress\", gave in 1949 a beautiful version of the novel). <br /><br />Anyway, the movie completely betrays the spirit of this work of the great American writer. In the novel, the heroine Catherine is shy, not very attractive and somewhat clumsy, but nonetheless she is a sound, intelligent young woman, and she's not as naive as it may seem. Her attachment for her father is dignified and respectful, with no morbid sides in it. Along three quarters of the movie, Catherine (Jennifer Jason Leigh) just seems to be mentally retarded, poor thing. In the last quarter, she suddenly (and incredibly) becomes intelligent, aware of her dignity as a woman, and all that.<br /><br />The director Agnieszka Holland has added two vulgar scenes to the story. The first, when the nervous child Catherine has, well, troubles with her vesica. The second scene, when we see on the background a sort of open-air brothel, with prostitutes taking their customers behind tents, and so on. Nothing could be more contrary to the spirit and artistic ideals of Henry James. It is notorious that the writer was extremely decent and demure even for the standards of the Victorian age. I defy anyone to find any coarseness anywhere in the thousands of pages of James' huge literary production. I really was particularly annoyed by these two scenes.<br /><br />Yes, I know that a director needs reasonable freedom in the screen adaptation of a novel. But if a director utterly ignores or misunderstands the art of an author (here Henry James), I don't see the point of using his work to make a bad movie. <br /><br />The acting is adequate to the movie: poor and flat, in spite of the talent of Albert Finney and Maggie Smith. \"Washington Square\" is definitely a non-recommendable film.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re  # Add this import for the 're' module\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_text(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "# Stopword removal function\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Stemming function\n",
        "def stem_text(tokens):\n",
        "    porter = PorterStemmer()\n",
        "    return [porter.stem(token) for token in tokens]\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "train_data['clean_text'] = train_data['text'].apply(clean_text)\n",
        "train_data['tokens'] = train_data['clean_text'].apply(tokenize_text)\n",
        "train_data['tokens'] = train_data['tokens'].apply(remove_stopwords)\n",
        "train_data['stemmed_tokens'] = train_data['tokens'].apply(stem_text)\n",
        "\n",
        "# Display the preprocessed data\n",
        "print(train_data[['text', 'clean_text', 'tokens', 'stemmed_tokens']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEG5Lo5mxuMq",
        "outputId": "acea79ba-ef93-42a5-8540-44d414f73b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "<ipython-input-7-261c43bcfa83>:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, 'html.parser').get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  Street Fight is a brilliant piece of brutal sa...   \n",
            "1  I just came back from the Late-night cinema an...   \n",
            "2  I sat with my children as we watched this film...   \n",
            "3  The Straight Story is the tale of an old man w...   \n",
            "4  Okay, note to the people that put together the...   \n",
            "\n",
            "                                          clean_text  \\\n",
            "0  street fight is a brilliant piece of brutal sa...   \n",
            "1  i just came back from the latenight cinema and...   \n",
            "2  i sat with my children as we watched this film...   \n",
            "3  the straight story is the tale of an old man w...   \n",
            "4  okay note to the people that put together thes...   \n",
            "\n",
            "                                              tokens  \\\n",
            "0  [street, fight, brilliant, piece, brutal, sati...   \n",
            "1  [came, back, latenight, cinema, indeed, silent...   \n",
            "2  [sat, children, watched, film, found, entertai...   \n",
            "3  [straight, story, tale, old, man, decides, vis...   \n",
            "4  [okay, note, people, put, together, horror, ac...   \n",
            "\n",
            "                                      stemmed_tokens  \n",
            "0  [street, fight, brilliant, piec, brutal, satir...  \n",
            "1  [came, back, latenight, cinema, inde, silent, ...  \n",
            "2  [sat, children, watch, film, found, entertain,...  \n",
            "3  [straight, stori, tale, old, man, decid, visit...  \n",
            "4  [okay, note, peopl, put, togeth, horror, act, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert the cleaned and preprocessed text data to TF-IDF vectors\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the max_features parameter based on your needs\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data['clean_text'])\n",
        "\n",
        "# Display the shape of the TF-IDF matrix\n",
        "print(\"Shape of TF-IDF matrix:\", X_train_tfidf.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94HTY1yQxwN1",
        "outputId": "423af83f-e91b-4e4a-9592-d9b0c5ad28a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of TF-IDF matrix: (25000, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the testing dataset\n",
        "test_data['clean_text'] = test_data['text'].apply(clean_text)\n",
        "test_data['tokens'] = test_data['clean_text'].apply(tokenize_text)\n",
        "test_data['tokens'] = test_data['tokens'].apply(remove_stopwords)\n",
        "test_data['stemmed_tokens'] = test_data['tokens'].apply(stem_text)\n",
        "\n",
        "# Display the preprocessed testing data\n",
        "print(test_data[['text', 'clean_text', 'tokens', 'stemmed_tokens']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqKEZbmmxyGZ",
        "outputId": "e7b3c40d-5867-47a9-8155-5f83c2df1fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-261c43bcfa83>:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  text = BeautifulSoup(text, 'html.parser').get_text()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  \\\n",
            "0  This film was original in an unoriginal way. A...   \n",
            "1  An extremely dark and brooding show with an ex...   \n",
            "2  First off, I absolutely loved this movie. As a...   \n",
            "3  This is not the kind of movie that really meri...   \n",
            "4  Julie Andrews and Rock Hudson were great in th...   \n",
            "\n",
            "                                          clean_text  \\\n",
            "0  this film was original in an unoriginal way al...   \n",
            "1  an extremely dark and brooding show with an ex...   \n",
            "2  first off i absolutely loved this movie as a b...   \n",
            "3  this is not the kind of movie that really meri...   \n",
            "4  julie andrews and rock hudson were great in th...   \n",
            "\n",
            "                                              tokens  \\\n",
            "0  [film, original, unoriginal, way, although, ma...   \n",
            "1  [extremely, dark, brooding, show, excellent, c...   \n",
            "2  [first, absolutely, loved, movie, billy, cryst...   \n",
            "3  [kind, movie, really, merits, critical, attent...   \n",
            "4  [julie, andrews, rock, hudson, great, movie, m...   \n",
            "\n",
            "                                      stemmed_tokens  \n",
            "0  [film, origin, unorigin, way, although, mani, ...  \n",
            "1  [extrem, dark, brood, show, excel, cast, one, ...  \n",
            "2  [first, absolut, love, movi, billi, crystal, f...  \n",
            "3  [kind, movi, realli, merit, critic, attent, go...  \n",
            "4  [juli, andrew, rock, hudson, great, movi, musi...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the columns in your test_data DataFrame\n",
        "print(test_data.columns)\n",
        "# Check if 'clean_text' is present in the columns of test_data\n",
        "print('clean_text' in test_data.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVM-SDK_x9NO",
        "outputId": "356285e3-7c6e-43b7-e48e-c810f2648937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['text', 'label', 'clean_text', 'tokens', 'stemmed_tokens'], dtype='object')\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Define and initialize nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier using the TF-IDF vectors and corresponding labels\n",
        "nb_classifier.fit(X_train_tfidf, train_data['label'])\n",
        "\n",
        "\n",
        "# Transform the testing data using the same vectorizer\n",
        "X_test_tfidf = tfidf_vectorizer.transform(test_data['clean_text'])\n",
        "\n",
        "# Predict the labels for the testing data\n",
        "predictions = nb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the performance of the model on the testing data\n",
        "accuracy = accuracy_score(test_data['label'], predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Display classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_data['label'], predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30XcFDp9x_D9",
        "outputId": "27bcab74-5888-47c1-caea-8d3107589572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.83      0.86      0.84     12500\n",
            "         pos       0.85      0.82      0.84     12500\n",
            "\n",
            "    accuracy                           0.84     25000\n",
            "   macro avg       0.84      0.84      0.84     25000\n",
            "weighted avg       0.84      0.84      0.84     25000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q66nWlD3zUCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENSEMBLE METHODS : Random Forest and Gradient Boosting"
      ],
      "metadata": {
        "id": "fdldwRdE0gJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming you have the train_data and test_data DataFrames with 'clean_text' and 'label' columns\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data['clean_text'], train_data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the number of features\n",
        "\n",
        "# Transform the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Random Forest classifier\n",
        "rf_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Transform the testing data using the same vectorizer\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Predict the labels for the testing data\n",
        "rf_predictions = rf_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the performance of the Random Forest model\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(\"Random Forest Accuracy:\", rf_accuracy)\n",
        "\n",
        "# Display classification report for Random Forest\n",
        "print(\"\\nRandom Forest Classification Report:\")\n",
        "print(classification_report(y_test, rf_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWNloSPk0j_o",
        "outputId": "04876323-5564-4001-fc35-cb51dd90fcee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.8348\n",
            "\n",
            "Random Forest Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.83      0.84      0.83      2485\n",
            "         pos       0.84      0.83      0.83      2515\n",
            "\n",
            "    accuracy                           0.83      5000\n",
            "   macro avg       0.83      0.83      0.83      5000\n",
            "weighted avg       0.83      0.83      0.83      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.externals import joblib\n",
        "\n",
        "# Assuming you have the train_data and test_data DataFrames with 'clean_text' and 'label' columns\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data['clean_text'], train_data['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the number of features\n",
        "\n",
        "# Transform the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Create a Gradient Boosting classifier\n",
        "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the Gradient Boosting classifier\n",
        "gb_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Transform the testing data using the same vectorizer\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Predict the labels for the testing data\n",
        "gb_predictions = gb_classifier.predict(X_test_tfidf)\n",
        "\n",
        "model_filename = 'gradient_boosting_model.pkl'\n",
        "joblib.dump(gb_classifier, model_filename)\n",
        "\n",
        "# Save the TF-IDF vectorizer to a file\n",
        "vectorizer_filename = 'tfidf_vectorizer.pkl'\n",
        "joblib.dump(tfidf_vectorizer, vectorizer_filename)\n",
        "\n",
        "# Evaluate the performance of the Gradient Boosting model\n",
        "gb_accuracy = accuracy_score(y_test, gb_predictions)\n",
        "print(\"Gradient Boosting Accuracy:\", gb_accuracy)\n",
        "\n",
        "# Display classification report for Gradient Boosting\n",
        "print(\"\\nGradient Boosting Classification Report:\")\n",
        "print(classification_report(y_test, gb_predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERfpuFaG07K6",
        "outputId": "6f95e582-a95a-4faa-8b16-5c3ac249f1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Accuracy: 0.8182\n",
            "\n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.86      0.76      0.81      2485\n",
            "         pos       0.79      0.87      0.83      2515\n",
            "\n",
            "    accuracy                           0.82      5000\n",
            "   macro avg       0.82      0.82      0.82      5000\n",
            "weighted avg       0.82      0.82      0.82      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced neural network architecture** using TensorFlow's Keras API. In this case, we'll use a ***bidirectional LSTM layer***, which is a type of recurrent neural network (RNN)."
      ],
      "metadata": {
        "id": "kAAkfgLI4f82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assuming you have the train_data and test_data DataFrames with 'clean_text' and 'label' columns\n",
        "\n",
        "# Convert string labels to numerical format\n",
        "label_mapping = {'neg': 0, 'pos': 1}\n",
        "train_data['label_numeric'] = train_data['label'].map(label_mapping)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data['clean_text'], train_data['label_numeric'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the number of features\n",
        "\n",
        "# Transform the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Tokenize and pad the sequences for neural network input\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=200)  # Adjust maxlen as needed\n",
        "\n",
        "# Build a bidirectional LSTM neural network model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the neural network\n",
        "model.fit(X_train_padded, y_train, epochs=5, batch_size=64)\n",
        "\n",
        "# Transform the testing data using the same vectorizer and tokenization\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=200)\n",
        "\n",
        "# Predict the probabilities for the testing data\n",
        "lstm_probabilities = model.predict(X_test_padded)\n",
        "\n",
        "# Convert probabilities to class predictions using a threshold (e.g., 0.5)\n",
        "threshold = 0.5\n",
        "lstm_predictions = (lstm_probabilities > threshold).astype(int)\n",
        "\n",
        "# Evaluate the performance of the LSTM model\n",
        "lstm_accuracy = accuracy_score(y_test, lstm_predictions)\n",
        "print(\"LSTM Accuracy:\", lstm_accuracy)\n",
        "\n",
        "# Display classification report for LSTM\n",
        "print(\"\\nLSTM Classification Report:\")\n",
        "print(classification_report(y_test, lstm_predictions))\n",
        "\n",
        "# Save the model\n",
        "model.save('lstm_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgW-IdEB3qUq",
        "outputId": "4ef8e341-1f7c-4b2d-b145-e0ada0d61dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "313/313 [==============================] - 328s 1s/step - loss: 0.4409 - accuracy: 0.7901\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 291s 928ms/step - loss: 0.2836 - accuracy: 0.8852\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 292s 934ms/step - loss: 0.2184 - accuracy: 0.9147\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 290s 928ms/step - loss: 0.1681 - accuracy: 0.9378\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 293s 935ms/step - loss: 0.1377 - accuracy: 0.9494\n",
            "157/157 [==============================] - 20s 121ms/step\n",
            "LSTM Accuracy: 0.853\n",
            "\n",
            "LSTM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85      2485\n",
            "           1       0.85      0.86      0.85      2515\n",
            "\n",
            "    accuracy                           0.85      5000\n",
            "   macro avg       0.85      0.85      0.85      5000\n",
            "weighted avg       0.85      0.85      0.85      5000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RfYS1J0x4nBa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}